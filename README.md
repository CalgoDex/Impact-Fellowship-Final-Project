# Impact-Fellowship-Final-Project
Echolocation band for the visually impaired final project for the Impact Fellowship Summit

At the end of this project, we hope to have a software that is capable of sensing the environment
around those whom are visually impaired and provide proper feedback on how to navigate this world.

There are 2 folders of interest (Analyzer and client), that house the required code for each device
in this process. There is an Emergency Routes script that, when fully implemented, should
be able to use ML algorithms to store emergency routes for exiting different buildings.

MVP: We are hoping that our minimum viable product will consist of an ideal set of sensory data that
will be fed into our analyzer, which will then determine which zone the object responsible for the
sensory data is in, then make the band vibrate in the general direction of the object.

We will be using the following libraries and sensory log data:

    Libraries
        1. REACT.JS
        2.
        3.
    Sensory logs
        1.
        2.
        3.

We used MongoDB as our backend

To run this application run Apps.jsx
